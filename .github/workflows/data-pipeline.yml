name: Data Pipeline

on:
  push:
    branches: [main]
    paths:
      - 'backend/**'
      - 'scripts/**'
      - '.github/workflows/data-pipeline.yml'
  workflow_dispatch:
    inputs:
      force_regenerate:
        description: 'Force regenerate all data'
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  pages: write
  id-token: write

env:
  PYTHON_VERSION: '3.9'

jobs:
  fetch-data:
    runs-on: ubuntu-slim
    outputs:
      data-hash: ${{ steps.compute-hash.outputs.hash }}
      should-skip: ${{ steps.check-changed.outputs.skip }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements-pipeline.txt

      - name: Install dependencies
        run: pip install -r backend/requirements-pipeline.txt

      - name: Restore previous data hash
        if: github.event.inputs.force_regenerate != 'true'
        id: cache-hash
        uses: actions/cache/restore@v4
        with:
          path: .data-hash
          key: durham-data-hash-latest
          restore-keys: durham-data-hash-

      - name: Fetch Durham census data
        env:
          CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}
        run: python scripts/fetch_durham_data.py

      - name: Generate crash data
        run: python scripts/fetch_ncdot_crash_data.py

      - name: Compute data hash
        id: compute-hash
        run: |
          HASH=$(cat \
            backend/data/raw/durham_census_tracts.geojson \
            backend/data/raw/ncdot_powerbi_response.json \
            2>/dev/null | sha256sum | cut -d' ' -f1 | cut -c1-16)
          echo "hash=$HASH" >> $GITHUB_OUTPUT
          echo "$HASH" > .data-hash

          if [ -f .data-hash ]; then
            cp .data-hash .data-hash.prev
          fi

      - name: Check if data changed
        id: check-changed
        run: |
          CURRENT="${{ steps.compute-hash.outputs.hash }}"
          FORCE="${{ github.event.inputs.force_regenerate }}"

          git fetch origin main --depth=1 2>/dev/null || true
          CODE_CHANGED=$(git diff --quiet HEAD origin/main -- backend/ scripts/ 2>/dev/null && echo "false" || echo "true")

          DATA_CHANGED="true"
          if [ -f .data-hash.prev ]; then
            PREV_HASH=$(cat .data-hash.prev)
            if [ "$CURRENT" == "$PREV_HASH" ]; then
              DATA_CHANGED="false"
            fi
          fi

          if [[ "${{ github.event_name }}" == "schedule" && "$DATA_CHANGED" == "false" && "$CODE_CHANGED" == "false" && "$FORCE" != "true" ]]; then
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Save data hash
        if: steps.check-changed.outputs.skip != 'true' && always()
        uses: actions/cache/save@v4
        with:
          path: .data-hash
          key: durham-data-hash-latest

      - name: Generate job summary
        run: |
          echo "## Durham Census Data Fetch" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f backend/data/raw/durham_census_tracts.geojson ]; then
            TRACTS=$(python3 -c "import json; d=json.load(open('backend/data/raw/durham_census_tracts.geojson')); print(len(d['features']))" 2>/dev/null || echo "N/A")
            SIZE=$(du -h backend/data/raw/durham_census_tracts.geojson | cut -f1)

            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Census Tracts | $TRACTS |" >> $GITHUB_STEP_SUMMARY
            echo "| File Size | $SIZE |" >> $GITHUB_STEP_SUMMARY
            echo "| Data Hash | \`${{ steps.compute-hash.outputs.hash }}\` |" >> $GITHUB_STEP_SUMMARY
            echo "| Timestamp | $(date -u +"%Y-%m-%d %H:%M:%S UTC") |" >> $GITHUB_STEP_SUMMARY

            if [ "${{ steps.check-changed.outputs.skip }}" == "true" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "> Skipping remaining pipeline: data unchanged, no code changes" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Upload source data artifact
        if: steps.check-changed.outputs.skip != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: raw-data
          path: backend/data/raw/
          retention-days: 7

  build:
    needs: fetch-data
    if: needs.fetch-data.outputs.should-skip != 'true'
    runs-on: ubuntu-slim
    env:
      DATA_HASH: ${{ needs.fetch-data.outputs.data-hash }}
      GITHUB_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
      REPO_URL: ${{ github.server_url }}/${{ github.repository }}
      GIT_SHA: ${{ github.sha }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements-pipeline.txt

      - name: Install Python dependencies
        run: pip install -r backend/requirements-pipeline.txt

      - name: Download census data
        uses: actions/download-artifact@v4
        with:
          name: raw-data
          path: backend/data/raw

      - name: Simulate volume predictions
        run: python scripts/simulate_ai_predictions.py

      - name: Simulate crash predictions
        run: python scripts/simulate_crash_predictions.py

      - name: Simulate infrastructure recommendations
        run: python scripts/simulate_infrastructure_recommendations.py

      - name: Analyze suppressed demand
        run: python scripts/analyze_suppressed_demand.py

      - name: Generate static JSON files
        run: |
          mkdir -p frontend/public/data
          python scripts/generate_static_data.py

          cat > frontend/public/data/metadata.json << EOF
          {
            "generated_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "data_hash": "${{ env.DATA_HASH }}",
            "github_run_url": "${{ env.GITHUB_RUN_URL }}",
            "git_sha": "${{ env.GIT_SHA }}"
          }
          EOF

      - name: Generate data summary
        run: |
          echo "## Static Files Generated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| File | Size |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|" >> $GITHUB_STEP_SUMMARY

          cd frontend/public/data
          for file in *.json; do
            SIZE=$(du -h "$file" | cut -f1)
            echo "| $file | $SIZE |" >> $GITHUB_STEP_SUMMARY
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Hash:** \`${{ env.DATA_HASH }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** [\`${GITHUB_SHA:0:7}\`](${{ env.REPO_URL }}/commit/${{ env.GIT_SHA }})" >> $GITHUB_STEP_SUMMARY

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Build frontend
        run: cd frontend && npm ci && GITHUB_PAGES=true npm run build

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: frontend/dist

  deploy:
    needs: build
    runs-on: ubuntu-slim

    permissions:
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
